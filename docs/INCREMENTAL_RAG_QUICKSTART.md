# Incremental RAG Quick Start Guide

## üöÄ 5-Minute Setup

### What You'll Build
A system that updates your RAG database incrementally - only processing new or changed documents instead of rebuilding everything.

### Prerequisites
- PCIe Debug Agent installed
- Python 3.8+
- 1GB+ free disk space

## Step 1: Basic Implementation (2 minutes)

Create `incremental_rag.py`:

```python
from pathlib import Path
import json
import hashlib
from datetime import datetime
from src.vectorstore.faiss_store import FAISSVectorStore
from src.models.embedding_selector import get_embedding_selector
from src.processors.document_chunker import DocumentChunker

class SimpleIncrementalRAG:
    def __init__(self, db_path="data/vectorstore"):
        self.db_path = Path(db_path)
        self.tracker_file = self.db_path / "tracker.json"
        self.tracker = self._load_tracker()
        
    def _load_tracker(self):
        if self.tracker_file.exists():
            return json.load(open(self.tracker_file))
        return {"docs": {}}
    
    def _save_tracker(self):
        self.tracker_file.parent.mkdir(exist_ok=True)
        json.dump(self.tracker, open(self.tracker_file, 'w'))
    
    def _hash_file(self, path):
        return hashlib.md5(open(path, 'rb').read()).hexdigest()
    
    def update(self, docs_dir="data/knowledge_base"):
        # Load or create vector store
        embedding = get_embedding_selector().get_current_provider()
        if (self.db_path / "index.faiss").exists():
            store = FAISSVectorStore.load(str(self.db_path))
        else:
            store = FAISSVectorStore(dimension=embedding.get_dimension())
        
        # Find new/changed files
        new_files = []
        for file in Path(docs_dir).glob("*.md"):
            hash = self._hash_file(file)
            if str(file) not in self.tracker["docs"] or \
               self.tracker["docs"][str(file)] != hash:
                new_files.append(file)
                self.tracker["docs"][str(file)] = hash
        
        # Process new files
        if new_files:
            chunker = DocumentChunker()
            for file in new_files:
                print(f"Processing: {file.name}")
                text = file.read_text()
                chunks = chunker.chunk_text(text)
                
                # Generate embeddings and add
                texts = [c['text'] for c in chunks]
                embeddings = embedding.encode(texts)
                metadata = [{"source": str(file)} for _ in chunks]
                
                store.add_documents(
                    embeddings.tolist(),
                    texts,
                    metadata
                )
            
            # Save everything
            store.save(str(self.db_path))
            self._save_tracker()
            print(f"‚úÖ Added {len(new_files)} documents")
        else:
            print("‚úÖ No updates needed")
```

## Step 2: Use It (1 minute)

```python
# First run - builds initial database
rag = SimpleIncrementalRAG()
rag.update()
# Output: Processing: doc1.md, doc2.md...
#         ‚úÖ Added 5 documents

# Second run - only new/changed files
rag.update()  
# Output: ‚úÖ No updates needed

# Add new file and run again
Path("data/knowledge_base/new.md").write_text("New content")
rag.update()
# Output: Processing: new.md
#         ‚úÖ Added 1 documents
```

## Step 3: Integration with PCIe Debug Agent (2 minutes)

### Add to Interactive Shell

```python
# In src/cli/interactive.py, add new command:

def do_update_kb(self, arg):
    """Update knowledge base incrementally"""
    from incremental_rag import SimpleIncrementalRAG
    
    print("üîÑ Checking for knowledge base updates...")
    rag = SimpleIncrementalRAG()
    rag.update()
```

### Add CLI Command

```python
# In src/cli/commands/vectordb.py, add:

@vectordb.command(name="update")
@click.option('--source', '-s', default='data/knowledge_base')
def incremental_update(source):
    """Incrementally update vector database"""
    from incremental_rag import SimpleIncrementalRAG
    
    rag = SimpleIncrementalRAG()
    rag.update(source)
```

## üéØ Real-World Examples

### Example 1: Auto-Update on File Change

```python
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class AutoUpdater(FileSystemEventHandler):
    def __init__(self):
        self.rag = SimpleIncrementalRAG()
        
    def on_modified(self, event):
        if event.src_path.endswith('.md'):
            print(f"üìù Detected change: {event.src_path}")
            self.rag.update()

# Watch for changes
observer = Observer()
observer.schedule(AutoUpdater(), "data/knowledge_base", recursive=True)
observer.start()
```

### Example 2: Scheduled Updates

```python
import schedule

def update_job():
    print("‚è∞ Running scheduled update...")
    SimpleIncrementalRAG().update()

# Update every hour
schedule.every().hour.do(update_job)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### Example 3: Git Hook Integration

Create `.git/hooks/post-commit`:
```bash
#!/bin/bash
echo "üîÑ Updating RAG database..."
python -c "from incremental_rag import SimpleIncrementalRAG; SimpleIncrementalRAG().update()"
```

## üìä Performance Comparison

| Operation | Traditional RAG | Incremental RAG |
|-----------|----------------|-----------------|
| Initial build (100 docs) | 60s | 60s |
| Add 1 doc | 60s (rebuild all) | 0.6s |
| Update 5 docs | 60s (rebuild all) | 3s |
| No changes | 60s (rebuild all) | 0.1s |

## üõ†Ô∏è Advanced Features

### Feature 1: Handle Deletions

```python
def update_with_deletions(self):
    # Track current files
    current_files = set(str(f) for f in Path("data/knowledge_base").glob("*.md"))
    tracked_files = set(self.tracker["docs"].keys())
    
    # Find deleted files
    deleted = tracked_files - current_files
    for file in deleted:
        print(f"üóëÔ∏è Removing: {file}")
        # In production: remove from vector store
        del self.tracker["docs"][file]
```

### Feature 2: Version History

```python
def update_with_history(self):
    if str(file) in self.tracker["docs"]:
        # Save old version
        version = self.tracker["docs"][str(file)].get("version", 0) + 1
        self.tracker["docs"][str(file)] = {
            "hash": hash,
            "version": version,
            "updated": datetime.now().isoformat()
        }
```

### Feature 3: Parallel Processing

```python
from concurrent.futures import ProcessPoolExecutor

def update_parallel(self, new_files):
    with ProcessPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(self.process_file, f) for f in new_files]
        for future in futures:
            future.result()
```

## üö® Common Issues & Solutions

### Issue 1: "Dimension mismatch"
```python
# Solution: Check embedding model hasn't changed
if store.dimension != embedding.get_dimension():
    print("‚ö†Ô∏è Embedding model changed - rebuild required")
    # Either rebuild or migrate
```

### Issue 2: "Out of memory"
```python
# Solution: Process in batches
BATCH_SIZE = 50
for i in range(0, len(new_files), BATCH_SIZE):
    batch = new_files[i:i+BATCH_SIZE]
    # Process batch...
```

### Issue 3: "Duplicate documents"
```python
# Solution: Add deduplication
seen_hashes = set()
for file in new_files:
    hash = self._hash_file(file)
    if hash not in seen_hashes:
        seen_hashes.add(hash)
        # Process file...
```

## üìã Checklist

- [ ] Created `incremental_rag.py`
- [ ] Tested basic update functionality
- [ ] Integrated with PCIe Debug Agent
- [ ] Set up auto-update (optional)
- [ ] Tested with your documents

## üéâ Next Steps

1. **Production Ready**: Add error handling, logging, and tests
2. **Scale Up**: Implement distributed locking for team use
3. **Optimize**: Add caching and parallel processing
4. **Monitor**: Track update metrics and performance

## üìö Resources

- [Full Implementation Guide](./INCREMENTAL_RAG_GUIDE.md)
- [API Reference](./API_REFERENCE.md)
- [Example Code](../examples/incremental_rag/)

---

**Questions?** Open an issue on [GitHub](https://github.com/together2329/pcie_debug_agent)