# PCIe Debug Agent - Local Model Configuration

## âœ… **Local Models Now Default**

The PCIe Debug Agent is now configured to use local models by default, requiring no API keys or internet connection.

## ðŸŽ¯ **Default Configuration**

### **1. Local LLM (Mock Model)**
```yaml
llm:
  provider: "local"
  model: "mock-llm"  # Built-in model, always available
```

**Features:**
- âœ… No download required
- âœ… Instant responses
- âœ… PCIe-specific knowledge
- âœ… Works completely offline

### **2. Local Embeddings**
```yaml
embedding:
  provider: "local"
  model: "sentence-transformers/all-MiniLM-L6-v2"
  dimension: 384
```

**Features:**
- âœ… Uses sentence-transformers
- âœ… No API keys needed
- âœ… 384-dimensional embeddings
- âœ… Fast local processing

## ðŸ“Š **Available Models**

```bash
./pcie-debug model list
```

| Model | Type | Status | Description |
|-------|------|--------|-------------|
| **mock-llm** âœ“ | Local | Always Ready | Built-in PCIe expert responses |
| llama-3.2-3b | Local | Needs download | Fast local inference |
| deepseek-r1-7b | Local | Needs download | Detailed reasoning |
| gpt-4 | API | Needs key | Best quality (online) |
| claude-3-opus | API | Needs key | Best quality (online) |

## ðŸš€ **Usage Examples**

### **Immediate Use (No Setup Required)**
```bash
# One-shot query
./pcie-debug -p "What causes PCIe completion timeout?"

# Interactive mode
./pcie-debug
ðŸ”§ > How to debug link training failures?
ðŸ”§ > /model  # Shows mock-llm as current
```

### **Mock Model Capabilities**
The built-in mock model provides expert responses for:
- âœ… Completion timeout errors
- âœ… Link training failures
- âœ… AER (Advanced Error Reporting)
- âœ… Power management (ASPM)
- âœ… Signal integrity issues
- âœ… General PCIe debugging

### **Example Response**
```bash
./pcie-debug -p "What causes PCIe AER errors?"

# Returns detailed response with:
# - Common AER error types
# - Root cause analysis
# - Debugging commands (lspci, setpci, etc.)
# - Resolution strategies
```

## ðŸ”§ **Configuration Files**

### **1. Main Config** (`configs/settings.yaml`)
```yaml
llm:
  provider: "local"
  model: "mock-llm"

embedding:
  provider: "local"
  model: "sentence-transformers/all-MiniLM-L6-v2"
```

### **2. Environment** (`.env`)
```bash
# Local models are default
EMBEDDING_PROVIDER=local
LLM_PROVIDER=local
LLM_MODEL=mock-llm

# Optional: Add if you want cloud models
# OPENAI_API_KEY=your-key
# ANTHROPIC_API_KEY=your-key
```

### **3. Model Settings** (`~/.pcie_debug/model_settings.json`)
```json
{
  "current_model": "mock-llm"
}
```

## ðŸ“‹ **Switching Models**

### **To Use Downloaded Models**
```bash
# Download model file to models/ directory
# Then switch:
./pcie-debug model set llama-3.2-3b
```

### **To Use API Models**
```bash
# Set API key
export OPENAI_API_KEY="your-key"

# Switch to API model
./pcie-debug model set gpt-4
```

### **To Return to Local**
```bash
./pcie-debug model set mock-llm
```

## ðŸŽ‰ **Benefits of Local-First Approach**

1. **No Setup Required**
   - Works immediately after installation
   - No API keys needed
   - No model downloads required

2. **Privacy & Security**
   - All processing happens locally
   - No data sent to cloud services
   - Suitable for sensitive logs

3. **Cost-Effective**
   - No API usage fees
   - No cloud costs
   - Unlimited queries

4. **Reliability**
   - Works offline
   - No network dependencies
   - Consistent availability

5. **Performance**
   - Instant responses with mock model
   - No network latency
   - Fast embedding generation

## ðŸ’¡ **Upgrade Path**

When you need more advanced capabilities:

1. **Download Local Models**
   ```bash
   # Get better local models
   # Place in models/ directory
   ./pcie-debug model set llama-3.2-3b
   ```

2. **Use Cloud Models**
   ```bash
   # For complex analysis
   export OPENAI_API_KEY="..."
   ./pcie-debug model set gpt-4
   ```

3. **Mix and Match**
   - Use mock for quick queries
   - Use cloud for complex analysis
   - Switch models on the fly

## âœ… **Current Status**

- **Default Model**: mock-llm (built-in)
- **Embeddings**: Local sentence-transformers
- **Vector Store**: 282 PCIe documents indexed
- **Status**: Fully operational, no external dependencies

The system is configured for immediate use with local models, providing expert PCIe debugging assistance without any setup requirements! ðŸš€