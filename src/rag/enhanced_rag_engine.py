"""
Enhanced RAG Engine with embedding-based LLM functionality
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from dataclasses import dataclass
import asyncio
from concurrent.futures import ThreadPoolExecutor
import json
from datetime import datetime

from src.vectorstore.faiss_store import FAISSVectorStore
from src.models.model_manager import ModelManager
from src.rag.retriever import Retriever
from src.rag.analyzer import Analyzer
from src.models.pcie_prompts import PCIePromptTemplates

logger = logging.getLogger(__name__)

@dataclass
class RAGQuery:
    """RAG ì¿¼ë¦¬ ê°ì²´"""
    query: str
    context_window: int = 5
    rerank: bool = True
    filters: Optional[Dict[str, Any]] = None
    min_similarity: float = 0.1  # Lowered from 0.5 to 0.1 for better recall
    include_metadata: bool = True

@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ê°ì²´"""
    answer: str
    sources: List[Dict[str, Any]]
    confidence: float
    reasoning: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class EnhancedRAGEngine:
    """í–¥ìƒëœ RAG ì—”ì§„ - Embedding ê¸°ë°˜ LLM í†µí•©"""
    
    def __init__(self,
                 vector_store: FAISSVectorStore,
                 model_manager: ModelManager,
                 llm_provider: str = "openai",
                 llm_model: str = "gpt-4",
                 temperature: float = 0.1,
                 max_tokens: int = 2000):
        
        self.vector_store = vector_store
        self.model_manager = model_manager
        self.llm_provider = llm_provider
        self.llm_model = llm_model
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.retriever = Retriever(vector_store, model_manager)
        
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.analyzer = Analyzer(
            llm_provider=llm_provider,
            model=llm_model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = {
            "queries_processed": 0,
            "average_response_time": 0,
            "average_confidence": 0,
            "cache_hits": 0
        }
        
        # ì¿¼ë¦¬ ìºì‹œ (ì„ íƒì‚¬í•­)
        self.query_cache = {}
        self.cache_size = 100
        
    def query(self, rag_query: RAGQuery) -> RAGResponse:
        """
        RAG ì¿¼ë¦¬ ì‹¤í–‰
        
        Args:
            rag_query: RAG ì¿¼ë¦¬ ê°ì²´
            
        Returns:
            RAG ì‘ë‹µ ê°ì²´
        """
        start_time = datetime.now()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._get_cache_key(rag_query)
            if cache_key in self.query_cache:
                self.metrics["cache_hits"] += 1
                cached_response = self.query_cache[cache_key]
                cached_response.metadata["from_cache"] = True
                return cached_response
            
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self._generate_query_embedding(rag_query.query)
            
            # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_docs = self._retrieve_documents(
                query_embedding,
                rag_query.context_window,
                rag_query.filters,
                rag_query.min_similarity
            )
            
            # 3. ë¬¸ì„œ ì¬ìˆœìœ„í™” (ì„ íƒì‚¬í•­)
            if rag_query.rerank and len(retrieved_docs) > 0:
                retrieved_docs = self._rerank_documents(
                    rag_query.query,
                    retrieved_docs
                )
            
            # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(retrieved_docs, rag_query.include_metadata)
            
            # 5. LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ë° í˜¸ì¶œ
            try:
                prompt = self._create_prompt(rag_query.query, context)
                llm_response = self._call_llm(prompt)
                answer, reasoning = self._parse_llm_response(llm_response)
            except Exception as e:
                # Fall back to search results only when LLM is unavailable
                logger.warning(f"LLM call failed, providing search results only: {e}")
                answer = self._create_fallback_answer(rag_query.query, retrieved_docs)
                reasoning = "LLM unavailable - providing search results from knowledge base only."
            
            # 7. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(retrieved_docs, answer)
            
            # 8. ì‘ë‹µ ê°ì²´ ìƒì„±
            response = RAGResponse(
                answer=answer,
                sources=retrieved_docs[:rag_query.context_window],
                confidence=confidence,
                reasoning=reasoning,
                metadata={
                    "query_time": (datetime.now() - start_time).total_seconds(),
                    "num_sources": len(retrieved_docs),
                    "model": self.llm_model,
                    "from_cache": False
                }
            )
            
            # ìºì‹œ ì €ì¥
            self._update_cache(cache_key, response)
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_metrics(response)
            
            return response
            
        except Exception as e:
            logger.error(f"Error in RAG query: {str(e)}")
            return RAGResponse(
                answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                sources=[],
                confidence=0.0,
                metadata={"error": str(e)}
            )
    
    async def aquery(self, rag_query: RAGQuery) -> RAGResponse:
        """ë¹„ë™ê¸° RAG ì¿¼ë¦¬ ì‹¤í–‰"""
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            response = await loop.run_in_executor(executor, self.query, rag_query)
        return response
    
    def batch_query(self, queries: List[RAGQuery]) -> List[RAGResponse]:
        """ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬"""
        responses = []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ì¼ê´„ ìƒì„±
        query_texts = [q.query for q in queries]
        query_embeddings = self.model_manager.generate_embeddings(query_texts)
        
        for query, embedding in zip(queries, query_embeddings):
            # ê°œë³„ ì¿¼ë¦¬ ì²˜ë¦¬ (ì„ë² ë”©ì€ ì¬ì‚¬ìš©)
            response = self._process_single_query_with_embedding(query, embedding)
            responses.append(response)
            
        return responses
    
    def _generate_query_embedding(self, query: str) -> np.ndarray:
        """ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±"""
        embeddings = self.model_manager.generate_embeddings([query])
        return embeddings[0]
    
    def _retrieve_documents(self,
                          query_embedding: np.ndarray,
                          k: int,
                          filters: Optional[Dict[str, Any]],
                          min_similarity: float) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        results = self.vector_store.search(query_embedding, k * 2)  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
        
        formatted_results = []
        for doc, metadata, score in results:
            # ìœ ì‚¬ë„ í•„í„°ë§
            if score < min_similarity:
                continue
                
            # ë©”íƒ€ë°ì´í„° í•„í„°ë§
            if filters:
                if not all(metadata.get(key) == value for key, value in filters.items()):
                    continue
            
            formatted_results.append({
                'content': doc,
                'metadata': metadata,
                'score': float(score)
            })
            
            if len(formatted_results) >= k:
                break
                
        return formatted_results
    
    def _rerank_documents(self, query: str, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì¬ìˆœìœ„í™”"""
        # Cross-encoder ë˜ëŠ” ë” ì •êµí•œ ì¬ìˆœìœ„ ëª¨ë¸ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬í˜„
        for doc in documents:
            # ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ì¶”ê°€ì ì¸ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            relevance_boost = self._calculate_relevance_boost(query, doc['content'])
            doc['rerank_score'] = doc['score'] * (1 + relevance_boost)
        
        # ì¬ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
        documents.sort(key=lambda x: x.get('rerank_score', x['score']), reverse=True)
        return documents
    
    def _calculate_relevance_boost(self, query: str, content: str) -> float:
        """ê´€ë ¨ì„± ë¶€ìŠ¤íŠ¸ ê³„ì‚°"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ë¶€ìŠ¤íŠ¸
        query_terms = set(query.lower().split())
        content_terms = set(content.lower().split())
        
        overlap = len(query_terms.intersection(content_terms))
        boost = overlap / len(query_terms) if query_terms else 0
        
        return boost * 0.5  # ìµœëŒ€ 50% ë¶€ìŠ¤íŠ¸
    
    def _build_context(self, documents: List[Dict[str, Any]], include_metadata: bool) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        for i, doc in enumerate(documents):
            context_part = f"[Source {i+1}]\n"
            
            if include_metadata and doc.get('metadata'):
                meta = doc['metadata']
                if 'file_name' in meta:
                    context_part += f"File: {meta['file_name']}\n"
                if 'section' in meta:
                    context_part += f"Section: {meta['section']}\n"
                if 'page' in meta:
                    context_part += f"Page: {meta['page']}\n"
            
            context_part += f"Content: {doc['content']}\n"
            context_part += f"Relevance Score: {doc['score']:.3f}\n"
            context_parts.append(context_part)
        
        return "\n---\n".join(context_parts)
    
    def _create_prompt(self, query: str, context: str) -> str:
        """LLM í”„ë¡¬í”„íŠ¸ ìƒì„± (PCIe íŠ¹í™”)"""
        # Use PCIe-specific prompt templates
        return PCIePromptTemplates.get_prompt_for_query_type(query, context)
    
    def _call_llm(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ"""
        return self.model_manager.generate_completion(
            provider=self.llm_provider,
            model=self.llm_model,
            prompt=prompt,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
    
    def _parse_llm_response(self, response: str) -> Tuple[str, Optional[str]]:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        # ì‘ë‹µì—ì„œ ë‹µë³€ê³¼ ì¶”ë¡  ë¶€ë¶„ ë¶„ë¦¬
        parts = response.split("Reasoning:")
        
        answer = parts[0].strip()
        reasoning = parts[1].strip() if len(parts) > 1 else None
        
        return answer, reasoning
    
    def _create_fallback_answer(self, query: str, documents: List[Dict[str, Any]]) -> str:
        """Create fallback answer when LLM is unavailable"""
        if not documents:
            return f"No relevant information found for query: '{query}'. Please check your question or try different keywords."
        
        # Remove duplicates based on content
        unique_docs = []
        seen_content = set()
        for doc in documents:
            content = doc.get('content', '')[:100]  # First 100 chars for comparison
            if content not in seen_content:
                unique_docs.append(doc)
                seen_content.add(content)
        
        answer = f"Search results for: '{query}'\n\n"
        answer += "Based on the PCIe knowledge base, here are the most relevant findings:\n\n"
        
        for i, doc in enumerate(unique_docs[:3], 1):  # Top 3 unique results
            content = doc.get('content', '')
            score = doc.get('score', 0.0)
            source = doc.get('source', 'Unknown source')
            
            answer += f"{i}. **Relevance: {score:.1%}**\n"
            answer += f"   Source: {source}\n"
            answer += f"   Content: {content[:300]}...\n\n"
        
        answer += "ğŸ’¡ **Recommendation**: For detailed analysis and solutions, please:\n"
        answer += "- Set up a local model (download required model files), or\n"
        answer += "- Configure API keys for cloud models (OpenAI/Anthropic)\n"
        answer += "- Use '/model list' to see available options"
        
        return answer

    def _calculate_confidence(self, documents: List[Dict[str, Any]], answer: str) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        if not documents:
            return 0.0
        
        # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜
        avg_score = np.mean([doc['score'] for doc in documents])
        
        # ë‹µë³€ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
        answer_length_factor = min(len(answer) / 500, 1.0)  # 500ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ
        
        # ì†ŒìŠ¤ ì¸ìš© í™•ì¸
        citation_factor = 1.0
        for i in range(len(documents)):
            if f"[Source {i+1}]" in answer:
                citation_factor += 0.1
        citation_factor = min(citation_factor, 1.5)
        
        # ìµœì¢… ì‹ ë¢°ë„
        confidence = avg_score * answer_length_factor * citation_factor
        return min(confidence, 1.0)
    
    def _get_cache_key(self, rag_query: RAGQuery) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_parts = [
            rag_query.query,
            str(rag_query.context_window),
            str(rag_query.filters),
            str(rag_query.min_similarity)
        ]
        return "|".join(key_parts)
    
    def _update_cache(self, key: str, response: RAGResponse):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        if len(self.query_cache) >= self.cache_size:
            # LRU ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self.query_cache))
            del self.query_cache[oldest_key]
        
        self.query_cache[key] = response
    
    def _update_metrics(self, response: RAGResponse):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.metrics["queries_processed"] += 1
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
        query_time = response.metadata.get("query_time", 0)
        prev_avg = self.metrics["average_response_time"]
        n = self.metrics["queries_processed"]
        self.metrics["average_response_time"] = (prev_avg * (n-1) + query_time) / n
        
        # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        prev_conf = self.metrics["average_confidence"]
        self.metrics["average_confidence"] = (prev_conf * (n-1) + response.confidence) / n
    
    def get_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.metrics.copy()
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.query_cache.clear()
        logger.info("Query cache cleared")
    
    def _process_single_query_with_embedding(self, 
                                           query: RAGQuery, 
                                           embedding: np.ndarray) -> RAGResponse:
        """ì„ë² ë”©ì´ ì£¼ì–´ì§„ ë‹¨ì¼ ì¿¼ë¦¬ ì²˜ë¦¬"""
        # ì´ë¯¸ ê³„ì‚°ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì²˜ë¦¬
        retrieved_docs = self._retrieve_documents(
            embedding,
            query.context_window,
            query.filters,
            query.min_similarity
        )
        
        if query.rerank and len(retrieved_docs) > 0:
            retrieved_docs = self._rerank_documents(query.query, retrieved_docs)
        
        context = self._build_context(retrieved_docs, query.include_metadata)
        prompt = self._create_prompt(query.query, context)
        llm_response = self._call_llm(prompt)
        answer, reasoning = self._parse_llm_response(llm_response)
        confidence = self._calculate_confidence(retrieved_docs, answer)
        
        return RAGResponse(
            answer=answer,
            sources=retrieved_docs[:query.context_window],
            confidence=confidence,
            reasoning=reasoning,
            metadata={
                "num_sources": len(retrieved_docs),
                "model": self.llm_model
            }
        ) 