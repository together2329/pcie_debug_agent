"""
UVM Debug Agent ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import argparse
import logging
from pathlib import Path
import sys
import os
import yaml
import streamlit as st

from config.settings import Settings, load_settings
from rag.enhanced_rag_engine import EnhancedRAGEngine
from ui.interactive_chat import InteractiveChatInterface
from ui.semantic_search import SemanticSearchInterface
from rag.vector_store import FAISSVectorStore
from rag.model_manager import ModelManager

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('logs/app.log')
        ]
    )

def load_app_settings() -> Settings:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_path = Path("configs/settings.yaml")
    
    if config_path.exists():
        settings = load_settings(config_path)
    else:
        # Create default settings with env vars
        print("Configuration file not found. Using default settings with environment variables.")
        settings = load_settings(None)
    
    settings.validate()
    return settings

def initialize_rag_engine(settings: Settings) -> EnhancedRAGEngine:
    """RAG ì—”ì§„ ì´ˆê¸°í™”"""
    try:
        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        vector_store = FAISSVectorStore(
            index_path=settings.vector_store.index_path,
            index_type=settings.vector_store.index_type,
            dimension=settings.vector_store.dimension
        )
        
        # ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        model_manager = ModelManager(
            embedding_model=settings.embedding.model,
            embedding_provider=settings.embedding.provider,
            embedding_api_key=settings.embedding.api_key,
            embedding_api_base_url=settings.embedding.api_base_url,
            llm_provider=settings.llm.provider,
            llm_model=settings.llm.model,
            llm_api_key=settings.llm.api_key,
            llm_api_base_url=settings.llm.api_base_url
        )
        
        # RAG ì—”ì§„ ì´ˆê¸°í™”
        engine = EnhancedRAGEngine(
            vector_store=vector_store,
            model_manager=model_manager,
            llm_provider=settings.llm.provider,
            llm_model=settings.llm.model,
            temperature=0.1,
            max_tokens=2000
        )
        return engine
    except Exception as e:
        logging.error(f"RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise

def run_web_interface(settings: Settings):
    """ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰"""
    st.set_page_config(
        page_title=settings.app_name,
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ì»¤ìŠ¤í…€ CSS
    st.markdown("""
        <style>
        .stApp {
            max-width: 1200px;
            margin: 0 auto;
        }
        .stButton>button {
            width: 100%;
        }
        </style>
    """, unsafe_allow_html=True)
    
    # RAG ì—”ì§„ ì´ˆê¸°í™”
    rag_engine = initialize_rag_engine(settings)
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.title("ğŸ” UVM Debug Agent")
        
        # ëª¨ë“œ ì„ íƒ
        mode = st.radio(
            "ëª¨ë“œ ì„ íƒ",
            ["Chat", "Search", "Settings"],
            index=0
        )
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        st.markdown("---")
        st.subheader("ì‹œìŠ¤í…œ ìƒíƒœ")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ì„ë² ë”© ëª¨ë¸", settings.embedding.model)
            st.metric("LLM ì œê³µì", settings.llm.provider)
        with col2:
            st.metric("ì²­í¬ í¬ê¸°", settings.rag.chunk_size)
            st.metric("ì´ ì¿¼ë¦¬ ìˆ˜", rag_engine.total_queries)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        st.markdown("---")
        st.subheader("ì„±ëŠ¥ ë©”íŠ¸ë¦­")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("í‰ê·  ì‘ë‹µ ì‹œê°„", f"{rag_engine.avg_response_time:.2f}s")
            st.metric("ìºì‹œ íˆíŠ¸", f"{rag_engine.cache_hits}")
        with col2:
            st.metric("í‰ê·  ì‹ ë¢°ë„", f"{rag_engine.avg_confidence:.2%}")
            st.metric("ìºì‹œ ë¯¸ìŠ¤", f"{rag_engine.cache_misses}")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if mode == "Chat":
        chat_interface = InteractiveChatInterface(rag_engine, settings)
        chat_interface.render()
    
    elif mode == "Search":
        search_interface = SemanticSearchInterface(rag_engine, settings)
        search_interface.render()
    
    else:  # Settings
        st.title("ì„¤ì •")
        
        with st.form("settings_form"):
            st.subheader("ì„ë² ë”© ì„¤ì •")
            embedding_provider = st.selectbox(
                "ì„ë² ë”© ì œê³µì",
                ["Local", "OpenAI", "Cohere", "Voyage", "Custom"],
                index=["Local", "OpenAI", "Cohere", "Voyage", "Custom"].index(settings.embedding.provider)
            )
            
            if embedding_provider == "Custom":
                embedding_model = st.text_input("ì„ë² ë”© ëª¨ë¸", settings.embedding.model)
                embedding_api_key = st.text_input("API í‚¤", settings.embedding.api_key or "", type="password")
                embedding_base_url = st.text_input("Base URL", settings.embedding.api_base_url or "")
                embedding_headers = st.text_area("ì¶”ê°€ í—¤ë” (JSON)", "{}")
            else:
                if embedding_provider == "OpenAI":
                    embedding_model = st.selectbox(
                        "ì„ë² ë”© ëª¨ë¸",
                        ["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"],
                        index=["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"].index(settings.embedding.model)
                    )
                elif embedding_provider == "Cohere":
                    embedding_model = st.selectbox(
                        "ì„ë² ë”© ëª¨ë¸",
                        ["embed-english-v3.0", "embed-multilingual-v3.0"],
                        index=["embed-english-v3.0", "embed-multilingual-v3.0"].index(settings.embedding.model)
                    )
                elif embedding_provider == "Voyage":
                    embedding_model = st.selectbox(
                        "ì„ë² ë”© ëª¨ë¸",
                        ["voyage-01", "voyage-02"],
                        index=["voyage-01", "voyage-02"].index(settings.embedding.model)
                    )
                else:
                    embedding_model = st.text_input("ì„ë² ë”© ëª¨ë¸", settings.embedding.model)
                
                embedding_api_key = st.text_input("API í‚¤", settings.embedding.api_key or "", type="password")
                embedding_base_url = st.text_input("Base URL (ì„ íƒì‚¬í•­)", settings.embedding.api_base_url or "")
                embedding_headers = "{}"
            
            st.subheader("LLM ì„¤ì •")
            llm_provider = st.selectbox(
                "LLM ì œê³µì",
                ["openai", "anthropic", "ollama", "custom"],
                index=["openai", "anthropic", "ollama", "custom"].index(settings.llm.provider)
            )
            
            if llm_provider == "custom":
                llm_model = st.text_input("LLM ëª¨ë¸", settings.llm.model)
                llm_api_key = st.text_input("API í‚¤", settings.llm.api_key or "", type="password")
                llm_base_url = st.text_input("Base URL", settings.llm.api_base_url or "")
                llm_headers = st.text_area("ì¶”ê°€ í—¤ë” (JSON)", "{}")
            else:
                if llm_provider == "openai":
                    llm_model = st.selectbox(
                        "LLM ëª¨ë¸",
                        ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"],
                        index=["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"].index(settings.llm.model)
                    )
                elif llm_provider == "anthropic":
                    llm_model = st.selectbox(
                        "LLM ëª¨ë¸",
                        ["claude-3-opus", "claude-3-sonnet", "claude-2"],
                        index=["claude-3-opus", "claude-3-sonnet", "claude-2"].index(settings.llm.model)
                    )
                elif llm_provider == "ollama":
                    llm_model = st.selectbox(
                        "LLM ëª¨ë¸",
                        ["llama2", "mistral", "codellama"],
                        index=["llama2", "mistral", "codellama"].index(settings.llm.model)
                    )
                else:
                    llm_model = st.text_input("LLM ëª¨ë¸", settings.llm.model)
                
                llm_api_key = st.text_input("API í‚¤", settings.llm.api_key or "", type="password")
                llm_base_url = st.text_input("Base URL (ì„ íƒì‚¬í•­)", settings.llm.api_base_url or "")
                llm_headers = "{}"
            
            st.subheader("RAG ì„¤ì •")
            chunk_size = st.number_input("ì²­í¬ í¬ê¸°", min_value=100, max_value=1000, value=settings.rag.chunk_size)
            chunk_overlap = st.number_input("ì²­í¬ ì˜¤ë²„ë©", min_value=0, max_value=100, value=settings.rag.chunk_overlap)
            context_window = st.number_input("ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°", min_value=1, max_value=10, value=settings.rag.context_window)
            min_similarity = st.slider("ìµœì†Œ ìœ ì‚¬ë„", min_value=0.0, max_value=1.0, value=settings.rag.min_similarity, step=0.05)
            rerank = st.checkbox("ì¬ìˆœìœ„í™”", value=settings.rag.rerank)
            
            st.subheader("UI ì„¤ì •")
            theme = st.selectbox("í…Œë§ˆ", ["light", "dark"], index=["light", "dark"].index(settings.ui.theme))
            max_width = st.number_input("ìµœëŒ€ ë„ˆë¹„", min_value=800, max_value=2000, value=settings.ui.max_width)
            show_confidence = st.checkbox("ì‹ ë¢°ë„ í‘œì‹œ", value=settings.ui.show_confidence)
            show_sources = st.checkbox("ì†ŒìŠ¤ í‘œì‹œ", value=settings.ui.show_sources)
            group_by_file = st.checkbox("íŒŒì¼ë³„ ê·¸ë£¹í™”", value=settings.ui.group_by_file)
            
            if st.form_submit_button("ì„¤ì • ì €ì¥"):
                # ì„¤ì • ì—…ë°ì´íŠ¸
                settings.embedding.provider = embedding_provider.lower()
                settings.embedding.model = embedding_model
                settings.embedding.api_key = embedding_api_key
                settings.embedding.api_base_url = embedding_base_url
                
                settings.llm.provider = llm_provider
                settings.llm.model = llm_model
                settings.llm.api_key = llm_api_key
                settings.llm.api_base_url = llm_base_url
                
                settings.rag.chunk_size = chunk_size
                settings.rag.chunk_overlap = chunk_overlap
                settings.rag.context_window = context_window
                settings.rag.min_similarity = min_similarity
                settings.rag.rerank = rerank
                
                settings.ui.theme = theme
                settings.ui.max_width = max_width
                settings.ui.show_confidence = show_confidence
                settings.ui.show_sources = show_sources
                settings.ui.group_by_file = group_by_file
                
                # ì„¤ì • ì €ì¥
                with open("configs/settings.yaml", 'w') as f:
                    yaml.dump(settings.to_dict(), f, default_flow_style=False)
                
                st.success("ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # RAG ì—”ì§„ ì¬ì´ˆê¸°í™”
                st.info("RAG ì—”ì§„ì„ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
                rag_engine = initialize_rag_engine(settings)
                st.success("RAG ì—”ì§„ì´ ì¬ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

def run_cli_interface(settings: Settings):
    """CLI ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰"""
    rag_engine = initialize_rag_engine(settings)
    
    print(f"Welcome to {settings.app_name} v{settings.version}")
    print("Type 'exit' to quit")
    
    while True:
        try:
            query = input("\nEnter your query: ")
            if query.lower() == 'exit':
                break
            
            response = rag_engine.query(query)
            print("\nResponse:", response.answer)
            
            if response.sources:
                print("\nSources:")
                for source in response.sources:
                    print(f"- {source['file']}: {source['content'][:100]}...")
            
            print(f"\nConfidence: {response.confidence:.2%}")
            print(f"Response time: {response.response_time:.2f}s")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"Error: {str(e)}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="UVM Debug Agent")
    parser.add_argument("--cli", action="store_true", help="Run in CLI mode")
    args = parser.parse_args()
    
    setup_logging()
    settings = load_app_settings()
    
    if args.cli:
        run_cli_interface(settings)
    else:
        run_web_interface(settings)

if __name__ == "__main__":
    main() 